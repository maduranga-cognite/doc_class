{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing & Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maduranga.J\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\cognite\\client\\_cognite_client.py:57: UserWarning: You are using version 1.1.1 of the SDK, however version 1.3.2 is available. Upgrade or set the environment variable 'COGNITE_DISABLE_PYPI_VERSION_CHECK' to suppress this warning.\n",
      "  debug=debug,\n"
     ]
    }
   ],
   "source": [
    "from cdf.connect import Con\n",
    "test = Con()\n",
    "test.getaccess()\n",
    "import pdf.read as f\n",
    "text = f.read_pdf(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "# tokens = word_tokenize(text)\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"python pythoner pythoning pythonly \\n \\nrun runner running ran\\n \\n  \\nword \\nc\\node\\nd\\n \\n \\n   \\ncoding coded\\n \\nd\\nogs \\n \\ncat\\ns cacti \\n \\n \\n'compute\\n'\\n'computer\\n'\\n'computed\\n'\\n'computing\\n'\\n \\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python pythoner pythoning pythonly run runner running ran word c ode d coding coded d ogs cat s cacti compute  computer  computed  computing'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = [\n",
    "    {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n",
    "    {r'\\s+': u' '},  # replace consecutive spaces\n",
    "    {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n",
    "    {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "    {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n",
    "    {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n",
    "    {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n",
    "    {r'[ \\t]*<[^<]*?/?>.,': u''},  # remove remaining tags\n",
    "    {r'^\\s+': u''},  # remove spaces at the beginning\n",
    "    {r',': u''},\n",
    "    {r'-': u''},\n",
    "    {r':': u''},\n",
    "    {r';': u''},\n",
    "    {r'\\d': u''},\n",
    "    {r'\\[.*?\\]': u''},\n",
    "    {r'\\(.*?\\)': u''},\n",
    "    {r'\\.': u''},\n",
    "    {r'\\'': u''}\n",
    "    \n",
    "    \n",
    "]\n",
    "for rule in rules:\n",
    "    for (k, v) in rule.items():\n",
    "        regex = re.compile(k)\n",
    "        text = regex.sub(v, text)\n",
    "text = text.rstrip()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'pythoner', 'pythoning', 'pythonly', 'run', 'runner', 'running', 'ran', 'word', 'c', 'ode', 'coding', 'coded', 'ogs', 'cat', 'cacti', 'compute', 'computer', 'computed', 'computing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(text)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency :  Counter({'python': 1, 'pythoner': 1, 'pythoning': 1, 'pythonly': 1, 'run': 1, 'runner': 1, 'running': 1, 'ran': 1, 'word': 1, 'c': 1, 'ode': 1, 'coding': 1, 'coded': 1, 'ogs': 1, 'cat': 1, 'cacti': 1, 'compute': 1, 'computer': 1, 'computed': 1, 'computing': 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "ctr = collections.Counter(filtered_sentence)\n",
    "print(\"Frequency : \",ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "stem_sentence=[]\n",
    "for word in filtered_sentence:\n",
    "    stem_sentence.append(porter.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'python', 'python', 'pythonli', 'run', 'runner', 'run', 'ran', 'word', 'c', 'ode', 'code', 'code', 'og', 'cat', 'cacti', 'comput', 'comput', 'comput', 'comput']\n"
     ]
    }
   ],
   "source": [
    "print(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem_sentence=[]\n",
    "for word in stem_sentence:\n",
    "    lem_sentence.append(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'python', 'python', 'pythonli', 'run', 'runner', 'run', 'ran', 'word', 'c', 'ode', 'code', 'code', 'og', 'cat', 'cactus', 'comput', 'comput', 'comput', 'comput']\n"
     ]
    }
   ],
   "source": [
    "print(lem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-idf with 75000 features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "def TFIDF(X_train, X_test, MAX_NB_WORDS=75000):\n",
    "    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
    "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
    "    X_test = vectorizer_x.transform(X_test).toarray()\n",
    "    print(\"tf-idf with\", str(np.array(X_train).shape[1]), \"features\")\n",
    "    return (X_train, X_test)\n",
    "\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "X_train,X_test = TFIDF(X_train,X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2000)\n",
    "X_train_new = pca.fit_transform(X_train)\n",
    "X_test_new = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import numpy\n",
    "# numpy.set_printoptions(threshold=sys.maxsize)\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train,X_test = TFIDF(X_train,X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
